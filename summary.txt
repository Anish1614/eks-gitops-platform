
---

# üöÄ Project Summary ‚Äî Production-Style Kubernetes Platform (Local First)

You designed and implemented a **GitOps-driven, autoscaling Kubernetes platform** using a local Kind cluster, structured exactly like a production AWS EKS environment.

---

# 1Ô∏è‚É£ Cluster Foundation

### Kubernetes Cluster

* Created multi-node cluster using **Kind**
* Structured namespaces:

  * `dev`
  * `prod`
  * `monitoring`
  * `argocd`

This mirrors real environment isolation in **Amazon EKS**.

---

# 2Ô∏è‚É£ GitOps Implementation

### Installed:

* **Argo CD**

### Achieved:

* Declarative deployment model
* Automated sync from GitHub
* Self-healing enabled
* No manual `kubectl apply` for applications

### Repository Structure (Industry-grade)

```text
eks-gitops-platform/
‚îú‚îÄ‚îÄ kubernetes/apps/nginx/
‚îú‚îÄ‚îÄ argocd/applications/
```

You moved from imperative deployment ‚Üí **declarative platform management**.

---

# 3Ô∏è‚É£ Application Deployment (Resource-Disciplined)

Deployed:

* NGINX microservice
* Defined CPU & memory requests/limits
* Structured with Kustomize
* Managed entirely via Git

This demonstrates:

* Resource governance
* Workload isolation
* Production readiness

---

# 4Ô∏è‚É£ Horizontal Pod Autoscaler (HPA)

Installed:

* Metrics Server

Configured:

* HPA targeting 50% CPU utilization
* Min replicas: 2
* Max replicas: 6

Tested:

* Load generation using busybox
* Observed automatic scaling

Result:

```
2 ‚Üí 3 ‚Üí 4 pods under load
```

You validated:

* CPU-based autoscaling
* Metrics pipeline functioning
* HPA reconciliation loop working

---

# 5Ô∏è‚É£ Observability Stack

Installed via Helm:

* **Prometheus**
* **Grafana**
* Alertmanager
* kube-state-metrics
* node-exporter

You now have:

* Cluster-level metrics
* Node-level metrics
* Namespace-level dashboards
* Pod-level resource visibility
* Real-time CPU spikes under load

This transitions you into **observability engineering**.

---


# =============================================================================
# üöÄ PRODUCTION-STYLE KUBERNETES PLATFORM - COMPLETE COMMAND REFERENCE
# =============================================================================

# =============================================================================
# PHASE 1: CLUSTER FOUNDATION (Kind Cluster Setup)
# =============================================================================

# Create Kind cluster with config
cat <<EOF > kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: prod-platform
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF

kind create cluster --config kind-config.yaml

# Verify cluster
kubectl cluster-info
kubectl get nodes

# Create namespaces (mirrors EKS environment isolation)
kubectl create namespace dev
kubectl create namespace prod
kubectl create namespace monitoring
kubectl create namespace argocd

# Verify namespaces
kubectl get namespaces

# =============================================================================
# PHASE 2: GITOPS IMPLEMENTATION (Argo CD Installation)
# =============================================================================

# Add Argo CD Helm repository
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

# Install Argo CD
helm install argocd argo/argo-cd -n argocd --create-namespace

# Wait for Argo CD to be ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s

# Port-forward Argo CD UI (run in separate terminal)
kubectl port-forward svc/argocd-server -n argocd 8080:443

# Get Argo CD initial admin password
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
echo

# Login to Argo CD CLI (optional)
argocd login localhost:8080 --username admin --password <password-from-above> --insecure

# =============================================================================
# PHASE 3: METRICS SERVER & HPA SETUP
# =============================================================================

# Install Metrics Server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# ISSUE: Readiness probe failed: HTTP probe failed with statuscode: 500
# FIX: Patch metrics-server for Kind cluster (insecure TLS)
kubectl patch deployment metrics-server -n kube-system --type='json' -p='[
  {
    "op": "add",
    "path": "/spec/template/spec/containers/0/args/-",
    "value": "--kubelet-insecure-tls"
  }
]'

# Wait for rollout
kubectl rollout status deployment/metrics-server -n kube-system

# Verify metrics-server
kubectl get pods -n kube-system | grep metrics
kubectl top nodes
kubectl top pods -n kube-system

# Create HPA for NGINX (example)
cat <<EOF > hpa-nginx.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
  namespace: dev
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 2
  maxReplicas: 6
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
EOF

kubectl apply -f hpa-nginx.yaml

# Verify HPA
kubectl get hpa -n dev
kubectl describe hpa nginx-hpa -n dev

# Load testing for HPA
kubectl run -i --tty load-generator --rm --image=busybox -n dev -- /bin/sh

# Inside the container, run:
while true; do wget -q -O- http://nginx-service; done

# Watch HPA scaling (in another terminal)
watch kubectl get hpa -n dev
watch kubectl get pods -n dev

# =============================================================================
# PHASE 4: OBSERVABILITY STACK (Prometheus + Grafana)
# =============================================================================

# Add Prometheus Helm repository
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Install kube-prometheus-stack
helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set grafana.enabled=true \
  --set prometheus.enabled=true \
  --set alertmanager.enabled=true

# Wait for all pods to be ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=grafana -n monitoring --timeout=300s
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=prometheus -n monitoring --timeout=300s

# Get Grafana admin password
kubectl --namespace monitoring get secrets monitoring-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo

# Port-forward Grafana (run in separate terminal)
kubectl port-forward svc/monitoring-grafana -n monitoring 3000:80

# Access Grafana: http://localhost:3000
# Username: admin
# Password: (from command above)

# Port-forward Prometheus (optional)
kubectl port-forward svc/monitoring-kube-prometheus-prometheus -n monitoring 9090:9090

# Access Prometheus: http://localhost:9090

# Useful monitoring queries in Grafana/Prometheus:
# - cluster_cpu_usage
# - cluster_memory_usage
# - node_cpu_seconds_total
# - container_cpu_usage_seconds_total
# - kube_pod_container_resource_requests
# - kube_horizontalpodautoscaler_status_current_replicas

# =============================================================================
# PHASE 5: APPLICATION DEPLOYMENT (GitOps Workflow)
# =============================================================================

# Directory structure setup
mkdir -p eks-gitops-platform/{kubernetes/apps/nginx,argocd/applications}

# NGINX Deployment with resource limits
cat <<EOF > eks-gitops-platform/kubernetes/apps/nginx/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: dev
  labels:
    app: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
EOF

# NGINX Service
cat <<EOF > eks-gitops-platform/kubernetes/apps/nginx/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: dev
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
EOF

# Kustomization
cat <<EOF > eks-gitops-platform/kubernetes/apps/nginx/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml
  - service.yaml
EOF

# Argo CD Application for NGINX
cat <<EOF > eks-gitops-platform/argocd/applications/nginx-dev.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: nginx-dev
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/YOUR_USERNAME/eks-gitops-platform.git
    targetRevision: HEAD
    path: kubernetes/apps/nginx
  destination:
    server: https://kubernetes.default.svc
    namespace: dev
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
EOF

# Apply Argo CD Application
kubectl apply -f eks-gitops-platform/argocd/applications/nginx-dev.yaml

# Verify application sync
argocd app get nginx-dev
kubectl get pods -n dev

# =============================================================================
# UTILITY COMMANDS
# =============================================================================

# Check cluster status
kubectl cluster-info
kubectl get nodes -o wide
kubectl get pods --all-namespaces

# Check Argo CD status
kubectl get pods -n argocd
kubectl get applications -n argocd

# Check HPA events
kubectl describe hpa nginx-hpa -n dev
kubectl get events -n dev --field-selector reason=ScalingReplicaSet

# View logs
kubectl logs -n argocd -l app.kubernetes.io/name=argocd-server
kubectl logs -n monitoring -l app.kubernetes.io/name=prometheus

# Port forwarding shortcuts
# Argo CD: kubectl port-forward svc/argocd-server -n argocd 8080:443
# Grafana: kubectl port-forward svc/monitoring-grafana -n monitoring 3000:80
# Prometheus: kubectl port-forward svc/monitoring-kube-prometheus-prometheus -n monitoring 9090:9090

# =============================================================================
# CLUSTER LIFECYCLE MANAGEMENT
# =============================================================================

# Stop Kind cluster (pause containers)
docker stop $(docker ps -q --filter "label=io.x-k8s.kind.cluster=prod-platform")

# Start Kind cluster (resume containers)
docker start $(docker ps -aq --filter "label=io.x-k8s.kind.cluster=prod-platform")

# Check cluster status
docker ps --filter "label=io.x-k8s.kind.cluster" --format "table {{.Names}}\t{{.Status}}"

# Delete Kind cluster (permanent)
kind delete cluster --name prod-platform

# Or delete all Kind clusters
kind delete clusters --all

# =============================================================================
# TROUBLESHOOTING
# =============================================================================

# Metrics Server issues
kubectl logs -n kube-system -l k8s-app=metrics-server
kubectl describe pod -n kube-system -l k8s-app=metrics-server

# HPA not scaling
kubectl describe hpa nginx-hpa -n dev
kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods

# Argo CD sync issues
argocd app sync nginx-dev
kubectl describe application nginx-dev -n argocd

# Resource usage
kubectl top nodes
kubectl top pods --all-namespaces
kubectl describe node <node-name>

# =============================================================================