# =============================================================================
# PRODUCTION-STYLE KUBERNETES PLATFORM - COMPLETE COMMAND REFERENCE
# =============================================================================

# =============================================================================
# PHASE 1: CLUSTER FOUNDATION (Kind Cluster Setup)
# =============================================================================
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.23.0/kind-linux-amd64
chmod +x kind
sudo mv kind /usr/local/bin/


# Create Kind cluster with config
cat <<EOF > kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: prod-platform
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF

kind create cluster --config kind-config.yaml


----------------
for local setup we don't need 2 worker node

# Delete current cluster
kind delete cluster --name prod-platform

# Create minimal cluster config
cat <<EOF > kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: prod-platform
nodes:
  - role: control-plane
  - role: worker
EOF

# Create new cluster
kind create cluster --config kind-config.yaml
------------------


# Verify cluster
kubectl cluster-info
kubectl get nodes

# Create namespaces (mirrors EKS environment isolation)
kubectl create namespace dev
kubectl create namespace prod
kubectl create namespace monitoring
kubectl create namespace argocd

# Verify namespaces
kubectl get namespaces

# =============================================================================
# PHASE 2: GITOPS IMPLEMENTATION (Argo CD Installation)
# =============================================================================

# Add Argo CD Helm repository
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

# Install Argo CD
helm install argocd argo/argo-cd -n argocd --create-namespace

# Wait for Argo CD to be ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s

# Port-forward Argo CD UI (run in separate terminal)
kubectl port-forward svc/argocd-server -n argocd 8080:443

# Get Argo CD initial admin password
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
echo


# =============================================================================
# PHASE 3 — Create First Microservice (Production-Style)
# =============================================================================
We will deploy:
Simple NGINX app
Through Git
Managed by ArgoCD

 
# PHASE : METRICS SERVER & HPA SETUP

# Install Metrics Server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# ISSUE: Readiness probe failed: HTTP probe failed with statuscode: 500
# FIX: Patch metrics-server for Kind cluster (insecure TLS)
kubectl patch deployment metrics-server -n kube-system --type='json' -p='[
  {
    "op": "add",
    "path": "/spec/template/spec/containers/0/args/-",
    "value": "--kubelet-insecure-tls"
  }
]'

# Wait for rollout
kubectl rollout status deployment/metrics-server -n kube-system

# Verify metrics-server
kubectl get pods -n kube-system | grep metrics
kubectl top nodes
kubectl top pods -n kube-system

# Create HPA for NGINX (example)

PHASE  — Simulate Load
kubectl get hpa -n dev
kubectl describe hpa nginx-hpa -n dev

# Load testing for HPA
kubectl run -i --tty load-generator --rm --image=busybox -n dev -- /bin/sh

# Inside the container, run:
while true; do wget -q -O- http://nginx-service; done

# Watch HPA scaling (in another terminal)
watch kubectl get hpa -n dev
watch kubectl get pods -n dev

# =============================================================================
# PHASE 4: OBSERVABILITY STACK (Prometheus + Grafana)
# =============================================================================

# Add Prometheus Helm repository
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Install kube-prometheus-stack
helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set grafana.enabled=true \
  --set prometheus.enabled=true \
  --set alertmanager.enabled=true
or simply 
helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace

# Wait for all pods to be ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=grafana -n monitoring --timeout=300s
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=prometheus -n monitoring --timeout=300s

# Get Grafana admin password
kubectl --namespace monitoring get secrets monitoring-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo

# Port-forward Grafana (run in separate terminal)
kubectl port-forward svc/monitoring-grafana -n monitoring 3000:80

# Access Grafana: http://localhost:3000
# Username: admin
# Password: (from command above)

# Port-forward Prometheus (optional)
kubectl port-forward svc/monitoring-kube-prometheus-prometheus -n monitoring 9090:9090

# Access Prometheus: http://localhost:9090

# Useful monitoring queries in Grafana/Prometheus:
# - cluster_cpu_usage
# - cluster_memory_usage
# - node_cpu_seconds_total
# - container_cpu_usage_seconds_total
# - kube_pod_container_resource_requests
# - kube_horizontalpodautoscaler_status_current_replicas



# =============================================================================
# PHASE 5 — Alerting & Incident Simulation
# =============================================================================
We will:

1️ Create custom alert rule (High CPU)
2️ Trigger alert intentionally
3️ Observe it in Alertmanager
4️ Simulate OOM kill
5️ Observe restart metrics

kubectl get pods -n monitoring | grep alertmanager
kubectl port-forward svc/monitoring-kube-prometheus-alertmanager -n monitoring 9093

Create Custom High CPU Alert
Trigger Alert
  - Run load generator again
Simulate OOM Kill = make memory very low and run load generation again

# =============================================================================
# PHASE 6 — Centralized Logging (Real Platform Capability)
# =============================================================================
Now we add log aggregation using:
Grafana
Loki (log backend)
Promtail (log collector)

install Loki
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
helm install loki grafana/loki-stack \
  --namespace monitoring


# =============================================================================
# UTILITY COMMANDS
# =============================================================================

# Check cluster status
kubectl cluster-info
kubectl get nodes -o wide
kubectl get pods --all-namespaces

# Check Argo CD status
kubectl get pods -n argocd
kubectl get applications -n argocd

# Check HPA events
kubectl describe hpa nginx-hpa -n dev
kubectl get events -n dev --field-selector reason=ScalingReplicaSet

# View logs
kubectl logs -n argocd -l app.kubernetes.io/name=argocd-server
kubectl logs -n monitoring -l app.kubernetes.io/name=prometheus

# Port forwarding shortcuts
# Argo CD: kubectl port-forward svc/argocd-server -n argocd 8080:443
# Grafana: kubectl port-forward svc/monitoring-grafana -n monitoring 3000:80
# Prometheus: kubectl port-forward svc/monitoring-kube-prometheus-prometheus -n monitoring 9090:9090

# =============================================================================
# CLUSTER LIFECYCLE MANAGEMENT
# =============================================================================

# Stop Kind cluster (pause containers)
docker stop $(docker ps -q --filter "label=io.x-k8s.kind.cluster=prod-platform")

# Start Kind cluster (resume containers)
docker start $(docker ps -aq --filter "label=io.x-k8s.kind.cluster=prod-platform")

# Check cluster status
docker ps --filter "label=io.x-k8s.kind.cluster" --format "table {{.Names}}\t{{.Status}}"

# Delete Kind cluster (permanent)
kind delete cluster --name prod-platform

# Or delete all Kind clusters
kind delete clusters --all

# =============================================================================
# TROUBLESHOOTING
# =============================================================================

# Metrics Server issues
kubectl logs -n kube-system -l k8s-app=metrics-server
kubectl describe pod -n kube-system -l k8s-app=metrics-server

# HPA not scaling
kubectl describe hpa nginx-hpa -n dev
kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods

# Argo CD sync issues
argocd app sync nginx-dev
kubectl describe application nginx-dev -n argocd

# Resource usage
kubectl top nodes
kubectl top pods --all-namespaces
kubectl describe node <node-name>

# =============================================================================